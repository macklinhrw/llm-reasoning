{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb7693d9-5fa2-4ffa-9a31-b5f1f3e37e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U flash-attn --no-build-isolation --no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3164194f-1079-462e-9396-6e9a5d77d469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b123d9a-c3cf-44db-a5fb-8977da1aed0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Nov 15 04:57:46 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.127.05             Driver Version: 550.127.05     CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A100-SXM4-80GB          Off |   00000000:81:00.0 Off |                    0 |\n",
      "| N/A   29C    P0             66W /  500W |       1MiB /  81920MiB |      0%      Default |\n",
      "|                                         |                        |             Disabled |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f65046-4496-42c0-a892-757328dc3a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get token\n",
    "hf_token = os.environ['HUGGING_FACE_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7eb52e68-08ac-4ca7-9e4e-5a928e923509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_chat_template(model_name):\n",
    "    try:\n",
    "        # Try to get the default template from the model's config\n",
    "        config = AutoConfig.from_pretrained(model_name)\n",
    "        if hasattr(config, \"chat_template\"):\n",
    "            return config.chat_template\n",
    "        \n",
    "        # Check if it's a known model with a specific template\n",
    "        if \"mistral\" in model_name.lower():\n",
    "            return \"<s>[INST] {{ if system }} {{ system }} {{ end }}{{ query }} [/INST] {{ response }} </s>\"\n",
    "        elif \"llama-2\" in model_name.lower():\n",
    "            return \"[INST] {% if system %} {{ system }} {% endif %} {{ user }} [/INST] {{ assistant }}\"\n",
    "        elif \"phi\" in model_name.lower():\n",
    "            return \"Instruct: {{ user }}\\nOutput: {{ assistant }}\"\n",
    "        else:\n",
    "            # Fallback to a generic template\n",
    "            return \"\"\"{% for message in messages %}{% if message['role'] == 'system' %}System: {{ message['content'] }}\n",
    "{% elif message['role'] == 'user' %}User: {{ message['content'] }}\n",
    "{% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}\n",
    "{% endif %}{% endfor %}{% if add_generation_prompt %}Assistant:{% endif %}\"\"\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error getting chat template: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def parse_number(text):\n",
    "    \"\"\"Parse a number string, handling commas.\"\"\"\n",
    "    try:\n",
    "        return float(text.replace(',', '').strip())\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing number: {text}\")\n",
    "        raise e\n",
    "\n",
    "def extract_answer(response):\n",
    "    \"\"\"Extract the final numerical answer from the response, handling commas.\"\"\"\n",
    "    try:\n",
    "        response = response.replace(',', '')\n",
    "        numbers = re.findall(r'-?\\d*\\.?\\d+', response)\n",
    "        return float(numbers[-1]) if numbers else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting answer from: {response}\")\n",
    "        return None\n",
    "\n",
    "def find_optimal_batch_size(model, tokenizer, max_batch_size=64, initial_batch_size=32):\n",
    "    print(\"Finding optimal batch size...\")\n",
    "    \n",
    "    sample_messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful math assistant. Solve the problem step by step.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is 2+2?\"}\n",
    "    ]\n",
    "    sample_text = tokenizer.apply_chat_template(\n",
    "        sample_messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    current_batch_size = initial_batch_size\n",
    "    \n",
    "    while current_batch_size <= max_batch_size:\n",
    "        try:\n",
    "            print(f\"Testing batch size: {current_batch_size}\")\n",
    "            batch_texts = [sample_text] * current_batch_size\n",
    "            \n",
    "            batch_inputs = tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True\n",
    "            ).to(model.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                _ = model.generate(\n",
    "                    **batch_inputs,\n",
    "                    max_new_tokens=512,\n",
    "                    do_sample=False,\n",
    "                    num_beams=1,\n",
    "                    pad_token_id=tokenizer.pad_token_id\n",
    "                )\n",
    "            \n",
    "            if current_batch_size == max_batch_size:\n",
    "                break\n",
    "            current_batch_size += 8\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            current_batch_size -= 8\n",
    "            break\n",
    "            \n",
    "    print(f\"Optimal batch size found: {current_batch_size}\")\n",
    "    return current_batch_size\n",
    "\n",
    "def print_gpu_memory():\n",
    "    allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "    print(f\"GPU Memory: Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
    "\n",
    "def batch_evaluate_gsm8k(model, tokenizer, batch_size=8, num_samples=None):\n",
    "    dataset = load_dataset(\"gsm8k\", \"main\")[\"test\"]\n",
    "    \n",
    "    if num_samples:\n",
    "        dataset = dataset.select(range(num_samples))\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    print(f\"Starting evaluation with batch size: {batch_size}\")\n",
    "    \n",
    "    results_log = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(dataset), batch_size)):\n",
    "        batch_slice = slice(i, min(i + batch_size, len(dataset)))\n",
    "        batch_data = dataset[batch_slice]\n",
    "        \n",
    "        questions = batch_data['question']\n",
    "        correct_answers = [parse_number(answer.split('####')[-1].strip()) \n",
    "                         for answer in batch_data['answer']]\n",
    "        \n",
    "        if isinstance(questions, str):\n",
    "            questions = [questions]\n",
    "            correct_answers = [correct_answers]\n",
    "        \n",
    "        batch_messages = [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful math assistant. Solve the problem step by step and provide the final answer as a number.\"},\n",
    "                {\"role\": \"user\", \"content\": question}\n",
    "            ]\n",
    "            for question in questions\n",
    "        ]\n",
    "        \n",
    "        batch_texts = [\n",
    "            tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            for messages in batch_messages\n",
    "        ]\n",
    "        \n",
    "        batch_inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True\n",
    "        ).to(model.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                **batch_inputs,\n",
    "                max_new_tokens=512,\n",
    "                do_sample=False,\n",
    "                num_beams=1,\n",
    "                pad_token_id=tokenizer.pad_token_id\n",
    "            )\n",
    "        \n",
    "        for j, (input_ids, output_ids) in enumerate(zip(batch_inputs.input_ids, generated_ids)):\n",
    "            generated_part = output_ids[len(input_ids):]\n",
    "            response = tokenizer.decode(generated_part, skip_special_tokens=True)\n",
    "            \n",
    "            predicted_answer = extract_answer(response)\n",
    "            \n",
    "            results_log.append({\n",
    "                'question': questions[j],\n",
    "                'response': response,\n",
    "                'predicted': predicted_answer,\n",
    "                'correct': correct_answers[j]\n",
    "            })\n",
    "            \n",
    "            if predicted_answer is not None:\n",
    "                if abs(predicted_answer - correct_answers[j]) < 1e-6:\n",
    "                    correct += 1\n",
    "            \n",
    "            total += 1\n",
    "        \n",
    "        if (i + batch_size) % (batch_size * 5) == 0:\n",
    "            print(f\"Progress: {total}/{len(dataset)} - Current Accuracy: {(correct/total)*100:.2f}%\")\n",
    "    \n",
    "    import json\n",
    "    with open('evaluation_results.json', 'w') as f:\n",
    "        json.dump(results_log, f, indent=2, default=str)\n",
    "    \n",
    "    final_accuracy = (correct / total) * 100\n",
    "    return final_accuracy\n",
    "\n",
    "def run_evaluation(model_name, num_samples=None):\n",
    "    # Enable Flash Attention\n",
    "    has_flash_attn = True\n",
    "    \n",
    "    # Configure model with Flash Attention\n",
    "    config = AutoConfig.from_pretrained(model_name)\n",
    "    if has_flash_attn:\n",
    "        if hasattr(config, \"use_flash_attention_2\"):\n",
    "            config.use_flash_attention_2 = True\n",
    "        if hasattr(config, \"attention_mode\"):\n",
    "            config.attention_mode = \"flash_attention_2\"\n",
    "    \n",
    "    # Load model with optimizations\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        config=config,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = 'left'\n",
    "\n",
    "    \n",
    "    chat_template = get_default_chat_template(model_name)\n",
    "    if chat_template:\n",
    "        tokenizer.chat_template = chat_template\n",
    "    else:\n",
    "        print(\"Warning: Using fallback chat template\")\n",
    "        tokenizer.chat_template = \"\"\"{% for message in messages %}{% if message['role'] == 'system' %}System: {{ message['content'] }}\n",
    "{% elif message['role'] == 'user' %}User: {{ message['content'] }}\n",
    "{% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}\n",
    "{% endif %}{% endfor %}{% if add_generation_prompt %}Assistant:{% endif %}\"\"\"\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "    # Find optimal batch size\n",
    "    batch_size = find_optimal_batch_size(model, tokenizer)\n",
    "    \n",
    "    # Print initial memory usage\n",
    "    print_gpu_memory()\n",
    "    \n",
    "    try:\n",
    "        accuracy = batch_evaluate_gsm8k(\n",
    "            model, \n",
    "            tokenizer, \n",
    "            batch_size=batch_size,\n",
    "            num_samples=num_samples\n",
    "        )\n",
    "        print(f\"\\nFinal Accuracy: {accuracy:.2f}%\")\n",
    "        return accuracy\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Evaluation failed with error: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dda0f358-294c-489a-965e-077a32bc3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['meta-llama/Llama-3.2-3B', 'Qwen/Qwen2.5-7B-Instruct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093e3b0a-ee16-4fd0-a5ff-bd9e7bad79e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cff773537b764811a152be2d90fcc1c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal batch size...\n",
      "Testing batch size: 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing batch size: 40\n",
      "Testing batch size: 48\n",
      "Testing batch size: 56\n",
      "Testing batch size: 64\n",
      "Optimal batch size found: 64\n",
      "GPU Memory: Allocated: 11.98GB, Reserved: 33.01GB\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c0b19841a4f48408078cde10f3b71ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/7.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbdeac0e24384ae392d485b4a8d08f33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/2.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "388048e347f8430498178ef44e592d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/419k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb398855cae4eb19da2b8cca43e01f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/7473 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459c9e0f423d4cbba622073f88d1d48b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/1319 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting evaluation with batch size: 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 5/21 [02:00<06:21, 23.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 320/1319 - Current Accuracy: 3.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 10/21 [04:00<04:23, 23.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 640/1319 - Current Accuracy: 3.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 12/21 [04:48<03:35, 23.99s/it]"
     ]
    }
   ],
   "source": [
    "model_name = models[0]\n",
    "\n",
    "accuracy = run_evaluation(\n",
    "    model_name=model_name,\n",
    "    num_samples=None  # Set to None for full dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b97ef4-7a8e-4d12-828b-b3c30a83c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GSM8K test dataset\n",
    "dataset = load_dataset(\"gsm8k\", \"main\")[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7391884c-e986-4e3f-8cc2-aaf2fd7343f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
