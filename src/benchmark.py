from datasets import load_dataset
from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
import torch
import re
import numpy as np
from tqdm import tqdm
import os
from dotenv import load_dotenv
from prompts import few_shot_prompt, zero_shot_prompt

# Load environment variables from .env file
load_dotenv()

# Get token from environment variables
hf_token = os.getenv("HUGGING_FACE_TOKEN")
if not hf_token:
    raise ValueError("HUGGING_FACE_TOKEN not found in environment variables")

os.environ["HF_TOKEN"] = hf_token


# Claude-3.5-sonnet generated code -- currently working on refactoring
def get_chat_template(model_name):
    try:
        # Try to get the default template from the model's config
        config = AutoConfig.from_pretrained(model_name)
        if hasattr(config, "chat_template"):
            return config.chat_template

    # These need need to be checked against official templates (these are generated by Claude)

    #         # Check if it's a known model with a specific template
    #         if "mistral" in model_name.lower():
    #             return "<s>[INST] {{ if system }} {{ system }} {{ end }}{{ query }} [/INST] {{ response }} </s>"
    #         elif "llama" in model_name.lower():
    #             return "[INST] {% if system %} {{ system }} {% endif %} {{ user }} [/INST] {{ assistant }}"
    #         elif "phi" in model_name.lower():
    #             return "Instruct: {{ user }}\nOutput: {{ assistant }}"
    #         else:
    #             # Fallback to a generic template
    #             return """{% for message in messages %}{% if message['role'] == 'system' %}System: {{ message['content'] }}
    # {% elif message['role'] == 'user' %}User: {{ message['content'] }}
    # {% elif message['role'] == 'assistant' %}Assistant: {{ message['content'] }}
    # {% endif %}{% endfor %}{% if add_generation_prompt %}Assistant:{% endif %}"""

    except Exception as e:
        print(f"Error getting chat template: {str(e)}")
        return None


def parse_number(text):
    """Parse a number string, handling commas."""
    try:
        return float(text.replace(",", "").strip())
    except Exception as e:
        print(f"Error parsing number: {text}")
        raise e


def extract_answer(response):
    """Extract the final numerical answer from the response, handling commas."""
    try:
        response = response.replace(",", "")
        # this regex seems fine for GSM8K
        numbers = re.findall(r"-?\d*\.?\d+", response)
        return float(numbers[-1]) if numbers else None
    except Exception as e:
        print(f"Error extracting answer from: {response}")
        return None


def batch_evaluate_gsm8k(
    model, tokenizer, batch_size=8, num_samples=None, prompt=zero_shot_prompt
):
    dataset = load_dataset("gsm8k", "main")["test"]

    if num_samples:
        dataset = dataset.select(range(num_samples))

    correct = 0
    total = 0

    print(f"Starting evaluation with batch size: {batch_size}")

    results_log = []

    for i in tqdm(range(0, len(dataset), batch_size)):
        batch_slice = slice(i, min(i + batch_size, len(dataset)))
        batch_data = dataset[batch_slice]

        questions = batch_data["question"]
        correct_answers = [
            parse_number(answer.split("####")[-1].strip())
            for answer in batch_data["answer"]
        ]

        if isinstance(questions, str):
            questions = [questions]
            correct_answers = [correct_answers]

        batch_messages = [
            [
                {
                    "role": "system",
                    "content": prompt,
                },
                {"role": "user", "content": question},
            ]
            for question in questions
        ]

        batch_texts = [
            tokenizer.apply_chat_template(
                messages, tokenize=False, add_generation_prompt=True
            )
            for messages in batch_messages
        ]

        # gives us a list of tensors, one for each question in the batch
        batch_inputs = tokenizer(
            batch_texts, return_tensors="pt", padding=True, truncation=True
        ).to(model.device)

        with torch.no_grad():
            # we can then pass this batch of tensors to the model
            # and it will generate in parallel for each question (batch inference)
            generated_ids = model.generate(
                **batch_inputs,
                max_new_tokens=512,
                do_sample=False,  # Greedy decoding
                num_beams=1,
                pad_token_id=tokenizer.pad_token_id,
            )

        for j, (input_ids, output_ids) in enumerate(
            zip(batch_inputs.input_ids, generated_ids)
        ):
            generated_part = output_ids[len(input_ids) :]
            response = tokenizer.decode(generated_part, skip_special_tokens=True)

            predicted_answer = extract_answer(response)

            results_log.append(
                {
                    "question": questions[j],
                    "response": response,
                    "predicted": predicted_answer,
                    "correct": correct_answers[j],
                }
            )

            if predicted_answer is not None:
                if abs(predicted_answer - correct_answers[j]) < 1e-6:
                    correct += 1

            total += 1

        # Print progress after each batch
        print(
            f"Progress: {total}/{len(dataset)} - Current Accuracy: {(correct/total)*100:.2f}%"
        )

    import json

    with open("evaluation_results.json", "w") as f:
        json.dump(results_log, f, indent=2, default=str)

    final_accuracy = (correct / total) * 100
    return final_accuracy


def run_evaluation(model_name, num_samples=None, batch_size=8, prompt=zero_shot_prompt):
    """
    Evaluates model accuracy on gsm8k dataset.
    """

    # Enable Flash Attention
    has_flash_attn = True

    # Configure model with Flash Attention
    config = AutoConfig.from_pretrained(model_name)
    if has_flash_attn:
        if hasattr(config, "use_flash_attention_2"):
            config.use_flash_attention_2 = True
        if hasattr(config, "attention_mode"):
            config.attention_mode = "flash_attention_2"

    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        config=config,
        torch_dtype=torch.float16,
        device_map="auto",
    )

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.padding_side = "left"

    # Instruct variants of models should already have a default chat template
    if tokenizer.chat_template is None:
        chat_template = get_chat_template(model_name)
        tokenizer.chat_template = chat_template

    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        model.config.pad_token_id = tokenizer.pad_token_id

    try:
        accuracy = batch_evaluate_gsm8k(
            model,
            tokenizer,
            batch_size=batch_size,
            num_samples=num_samples,
            prompt=prompt,
        )
        print(f"\nFinal Accuracy: {accuracy:.2f}%")
        return accuracy

    except Exception as e:
        print(f"Evaluation failed with error: {str(e)}")
        import traceback

        traceback.print_exc()
        return None


models = [
    "meta-llama/Llama-3.2-3B",
    "meta-llama/Llama-3.1-8B",
    "meta-llama/Llama-3.2-1B-Instruct",
    "meta-llama/Llama-3.2-3B-Instruct",
    "meta-llama/Llama-3.1-8B-Instruct",
    "Qwen/Qwen2.5-7B-Instruct",
]

if __name__ == "__main__":
    model_name = models[2]

    accuracy = run_evaluation(
        model_name=model_name,
        num_samples=None,
        batch_size=64,
        prompt=few_shot_prompt,
    )
